{
  "hash": "06e6ad2fec5aba1b6e69cda4c58c06ae",
  "result": {
    "markdown": "---\ntitle: \"Lesson 1\"\nformat: \n  html:\n    eval: false\n---\n\n\n## Set up an R Project\n\nAs a first step whenever you start a new project, workflow, analysis, etc., it is good practice to set up an R project. R Projects are RStudio's way of bundling together all your files for a specific project, such as data, scripts, results, figures. Your project directory also becomes your working directory, so everything is self-contained and easily portable.\n\nYou can start an R project in an existing directory or in a new one. To create a project go to File -\\> New Project:\n\n![](images/paste-050BDFEC.png)\n\nLet's create a new directory and call it 'R-for-Geospatial'. You can make it a sub directory of any folder you wish.\n\n![](images/paste-6F92D857.png)\n\nNow we are working in our R project. You can see the working directory printed at the top of your console is now our project directory, and in the 'Files' tab in RStudio you can see we have an .Rproj file, which will open up this R project in RStudio whenever you come back to it. For example close out of this R session, navigate to the project folder on your computer, and double-click the .Rproj file.\n\n## Read in R packages\n\nNow we have started a fresh R session in our new R project, we need to read in the libraries needed to work through today's lesson. You should have all packages installed after finishing the set-up instructions on the [Getting Started](setup.qmd) page.\n\nIn the set-up lesson, you used the following function to check if a package is installed, if not install it, and then load that package into your session.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npackageLoad <-\n  function(x) {\n    for (i in 1:length(x)) {\n      if (!x[i] %in% installed.packages()) {\n        install.packages(x[i])\n      }\n      library(x[i], character.only = TRUE)\n    }\n  }\n```\n:::\n\n\nWe will be using this function the rest of the workshop to read in a list of packages at the beginning of each lesson, so lets store it as its own R script that we can call in later with the `source()` function. Sourcing functions is good practice as it reduces repetitiveness of rewriting them every time you want to use it.\n\nNow let's use it to load in our libraries needed for today. Assuming you already installed all of these, loading them should run pretty quick.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npackageLoad(c(\"rgbif\", \"tidycensus\", \"tigris\", \"sf\", \"terra\", \"dplyr\", \"tidyr\", \"readr\"))\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nToday we are going to walk through how to import spatial data directly into R, clean it, and save it so we can work with it the rest of the workshop.\n\n\n::: {.cell}\n\n:::\n\n\nWe will work through how most of these data sets were imported and cleaned today, but not all of them are covered. To see how all data sets were created you can check out the [Pulling Data](getdata.qmd) page.\n\n## Spatial Data Formats\n\n**Vector Data**\n\n-   Locations (points)\n\n    -   Coordinates, address, country, city\n\n-   Shapes (lines or polygons)\n\n    -   Political boundaries, roads, building footprints, water bodies\n\n**Raster Data**\n\n-   Images (matrix of cells organized by rows and columns)\n\n    -   Satellite imagery, climate, landcover, elevation\n\n        ![](images/paste-571C0D9A.png){width=\"430\"}\n\n## Import and manipulate spatial data\n\nNow we have our project set up, packages loaded, and a basic understanding of the spatial data types we will be using. Let's get some data to work with! For the first part we are going to import spatial data into R using the `tigris`, `tidycensus`, and `rgbif` packages. The raster data has been pre-processed (see [Pulling Data](getdata.qmd) for how that was done in R) and we will read those objects in from file.\n\n**First, lets create a new folder in our project directory called 'data/' where we will be saving all the data used for this workshop.**\n\n### Vector Data\n\n#### `tigris`\n\nAll the data we are working with in this course is confined to the state of Colorado. Let's start by pulling in political boundaries for Colorado counties with the [`tigris`](https://github.com/walkerke/tigris) package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# download county shapefile for the state of Colorado\ncounties <- tigris::counties(state = \"CO\")\n```\n:::\n\n\n`tigris` has a lot of other available data sets in addition to political boundaries. In this course we are going to work with additional line shape files (rivers and roads) and polygons (urban areas).\n\nDue to the density of rivers and roads in Colorado and therefore large file size, we are going to limit this download to just Larimer County.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrivers <- linear_water(state = \"CO\", county = \"Larimer\")\n\nroads <- roads(state = \"CO\", county = \"Larimer\")\n```\n:::\n\n\nOur object `rivers` actually includes all linear water features in the county, such as streams/rivers, braided streams, canals, ditches, artificial paths, and aqueducts.\n\n`counties` is a spatial polygon data set. We are going to work with one more polygon data set which includes the boundaries of individual urban areas and clusters across Colorado. We can explore the `urban_areas()` function from the `tigris` package to get more details on how urban areas are defined.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n?urban_areas\n```\n:::\n\n\nNow let's download the data set. By default it imports urban areas for all of the U.S., but we just want areas within Colorado. We have to do some data wrangling to filter out just Colorado urban areas. The column \"NAME10\" has the city and state of each urban area. We can use the `separate()` function from the `tidyr` package to split the \"NAME10\" column into two separate columns for city and state, and then we can use the `filter()` function to subset just the urban areas in Colorado.\n\nWe are also using a new function here, the pipe `%>%` operator. This allows us to run a sequence of operations without having to create intermediate objects that take up unnecessary space in your environment. It can be interpreted as 'and then', taking the output of one function 'and then' running the next function using that output as the input. We will be using the pipe a lot throughout this workshop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurban <- urban_areas() %>% \n  tidyr::separate(col = NAME10, sep = \", \", into = c(\"city\", \"state\")) %>% \n  dplyr::filter(state == \"CO\")\n```\n:::\n\n\n#### `tidycensus`\n\n[`tidycensus`](https://walker-data.com/tidycensus/) is an R package that allows users to access U.S. Census data, imported as \"tidy\" data designed to work with tidyverse packages. The tidyverse is a collection of R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis. They all use the same design philosophy, grammar, and data structures. When you install the tidyverse, it installs all of these packages, and you can then load all of them in your R session with `library(tidyverse)`. You can learn more about the tidyverse and the packages it includes here: <https://www.tidyverse.org/>.\n\nThe `tidycensus` package requires the use of an API key to download data. You can obtain an API key for free at <http://api.census.gov/data/key_signup.html>\n\nOnce you have the API key, you need to run the following line of code before downloading any census data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus_api_key(\"PASTE YOUR API KEY HERE\")\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nNow let's first investigate what census variables are available (there's a lot). `tidycesus` has two major functions: `get_decennial()` to access the 2000, 2010, and 2020 decennial U.S. Census data, and `get_acs()` to access the 1-year and 5-year American Community Survey (ACS) data. For this workshop we are going to work with ACS data.\n\nWe can get the full list of available variables with the `load_variables()` function, and we are interested in the 5-year ACS data for 2019 (the most recent year available). We then pass this output to `View()` which will open the data frame in a separate tab to be viewed similar to an excel spreadsheet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload_variables(2019, \"acs5\") %>% View()\n```\n:::\n\n\nWe are going to download total population and median household income. We use the `get_acs()` function and specify we want this data at the county level for the state of Colorado and supply a string of the variables we want, using the variable ID.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus <- get_acs(geography = \"county\", state = \"CO\", year = 2019,\n                  variables = c(\"B01003_001\", \"B19013_001\"), output = \"wide\")\n```\n:::\n\n\nLet's clean this up a bit. \"E\" in the variable name stands for estimate and \"M\" is margin of error. For our purposes we are only looking at the estimate values, so lets remove the other columns and rename the ones we keep with more informative titles.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncensus <- census %>% \n  dplyr::select(contains(\"E\")) %>% \n  rename(total_pop = B01003_001E, med_income = B19013_001E)\n```\n:::\n\n\n#### `rgbif`\n\nThe last of our spatial data download is species occurrences in the form of point data (latitude/longitude). `rgbif` is a package that allows you to download species occurrences from the [Global Biodiversity Information Facility (GBIF)](https://www.gbif.org/), a database of global species occurrences with over 2.2 billion records.\n\nWe are going to pull occurrence data for a couple of charismatic Colorado species:\n\n|                              |                                    |                                            |\n|:------------------:|:----------------------:|:--------------------------:|\n| ![Elk](elk.jpg){width=\"173\"} | ![Marmot](marmot.jpg){width=\"173\"} | ![Salamander](salamander.jpg){width=\"215\"} |\n|             Elk              |       Yellow-Bellied Marmot        |          Western Tiger Salamander          |\n\nTo pull data for multiple species we are going to run the `occ_data()` function from the `rgbif` package over multiple species using a for loop. For a refresher on for loops see the [R Basics](basics.qmd) page.\n\nWe first need to create a string of species scientific names to use in the download function, and a second string with their associated common names.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#make a string of species names to use in the 'occ_data' function\nspecies <- c(\"Cervus canadensis\", \"Marmota flaviventris\", \"Ambystoma mavortium\")\n\n#also make a string of common names\ncommon_name <- c(\"Elk\", \"Yellow-bellied Marmot\", \"Western Tiger Salamander\")\n```\n:::\n\n\nNow we are going to write a for loop that iterates across our three species. We first create an empty list that is the length of our species vector that we will fill with each species data download output and bind all the data together outside of the loop.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nocc <- vector(\"list\", length = length(species)) \n\n\nfor(i in 1:length(occ)){\n  \n  occ[[i]] <-\n    occ_data(\n      scientificName = species[i],\n      hasCoordinate = TRUE,\n      geometry = st_bbox(counties),\n      limit = 2000\n    ) %>%\n    .$data #return just the data frame. The '.' symbolizes the previous function's output\n  \n  # add species name column as ID to use later\n  occ[[i]]$ID <- common_name[i]\n  \n  #clean by removing duplicate occurrences\n  occ[[i]] <-\n    occ[[i]] %>% distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) %>%\n    dplyr::select(Species = ID,\n           decimalLatitude,\n           decimalLongitude,\n           year,\n           month,\n           basisOfRecord) #only keep relevant variables\n  \n  \n  \n  print(i) # this prints each element once its finished so you can see the progress\n  \n}\n\n# Bind all data frames together\nocc <- bind_rows(occ)\n```\n:::\n\n\n### Raster Data\n\nThe raster files for this workshop have already been processed (see [Pulling Data](getdata.qmd) for how this was done). Click each button below to download the GeoTIFF files for elevation and land cover (plus the necessary land cover attribute file) and save them in your 'data/' folder within this project.\n\n::: {.callout-note appearance=\"minimal\"}\n<i class=\"bi bi-download\"></i> [Download Elevation Data](data_test/elevation_1km.tif){download=\"elevation_1km.tif\"}\n:::\n\n::: {.callout-note appearance=\"minimal\"}\n<i class=\"bi bi-download\"></i> [Download Land Cover Data](data_test/NLCD_CO.tif){download=\"NLCD_CO.tif\"}\n:::\n\n::: {.callout-note appearance=\"minimal\"}\n<i class=\"bi bi-download\"></i> [Download Land Cover Attribute Data](data_test/NLCD_CO.tif.aux.xml){download=\"NLCD_CO.tif.aux.xml\"}\n:::\n\nOnce saved, we can read in raster files using the `rast()` function from the `terra` package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelevation <- terra::rast(\"data/elevation_1km.tif\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nterra::plot(elevation)\n```\n\n::: {.cell-output-display}\n![](day1_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n## Coordinate Reference Systems\n\nProbably the most important part of working with spatial data is the coordinate reference system (CRS) that is used. In order to analyze and visualize spatial data, all objects must be in the exact same CRS.\n\nWe can check a spatial object's CRS by printing it to the console, which will print a bunch of metadata about the object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nelevation\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nclass       : SpatRaster \ndimensions  : 1322, 1724, 1  (nrow, ncol, nlyr)\nresolution  : 0.004895063, 0.004895063  (x, y)\nextent      : -109.6875, -101.2484, 36.59761, 43.06889  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : elevation_1km.tif \nname        : elevation_1km \nmin value   :           726 \nmax value   :          4288 \n```\n:::\n:::\n\n\nBefore we start performing spatial analyses in Lesson 2 we will need to return to the CRS of these spatial objects and make sure they all match. If you need to transform a spatial object to a different CRS you can use the `st_transform()` function from the `sf` package for vector data, and `project()` from the `terra` package for raster data.\n\n## Saving spatial data\n\nYou do not necessarily need to run this part, since the 'data/' folder we downloaded at the beginning of today's lesson contains all these data files we will need for tomorrow. However learning how to write spatial data to file is an important skill.\n\nTo save shapefiles, you can use the `st_write()` function from the `sf` package. If the intended file happens to already exists and you want to overwrite it, you would add `append = FALSE` to the argument.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_write(rivers, \"data/rivers.shp\")\n\nst_write(roads, \"data/roads.shp\")\n\nst_write(urban, \"data/urban_areas.shp\")\n\nst_write(counties, \"data/CO_counties.shp\")\n```\n:::\n\n\nWe can save point files as a csv with the `write_csv()` function. We will go over tomorrow how to read a csv file with coordinates and convert it into a spatial object.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_csv(census, \"data/census_data.csv\")\n\nwrite_csv(occ, \"data/species_occ.csv\")\n```\n:::\n",
    "supporting": [
      "day1_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}