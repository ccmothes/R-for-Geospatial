[
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "R Basics",
    "section": "",
    "text": "When you first open RStudio, it is split into 3 panels:\n\nThe Console (left), where you can directly type and run code (by hitting Enter)\nThe Environment/History pane (upper-right), where you can view the objects you currently have stored in your environment and a history of the code you’ve run\nThe Files/Plots/Packages/Help pane (lower-right), where you can search for files, view and save your plots, view and manage what packages are loaded in your library and session, and get R help\n\n\n\n\nImage Credit: Software Carpentry\n\n\nTo write and save code you use scripts. You can open a new script with File -> New File or by clicking the icon with the white square and green plus sign in the upper left corner. When you open a script, RStudio then opens a fourth ‘Source’ panel in the upper-left to write and save your code. You can also send code from a script directly to the console to execute it by highlighting the code line/chunk (or place your cursor at the end of the code chunk) and hit CTRL+ENTER on a PC or CMD+ENTER on a Mac.\n\n\n\nImage Credit: Software Carpentry\n\n\nIt is good practice to add comments/notes throughout your scripts to document what the code is doing. To do this start a line with a #. R knows to ignore everything after a #, so you can write whatever you want there.\n\n\nR has many built in functions to perform various tasks. To run these functions you type the function name followed by parentheses. Within the parentheses you put in your specific arguments needed to run the function.\n\n# mathematical functions with numbers\nlog(10)\n\n[1] 2.302585\n\n# a range of numbers\nmean(1:5)\n\n[1] 3\n\n# nested functions for a string of numbers, using the concatenate function 'c'\nmean(c(1,2,3,4,5))\n\n[1] 3\n\n# functions with characters\nprint(\"Hello World\")\n\n[1] \"Hello World\"\n\npaste(\"Hello\", \"World\")\n\n[1] \"Hello World\"\n\n\n\n\n\nR Packages include reusable functions that are not built-in with R. To use these functions, you must install the package to your local system with the install.packages() function. Once a package is installed on your computer you don’t need to install it again. Anytime you want to use the package in a new R session you just load it with the library() function. Go to the Getting Started page to walk through installing and loading the R packages needed for this workshop.\n\n\nYou may hear the word “tidy” a lot throughout this workshop, which is referring to the Tidyverse. The Tidyverse is a collection of R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis. They all use the same design philosophy, grammar, and data structures. When you install the Tidyverse, it installs all of these packages, and you can then load all of them in your R session with library(tidyverse). For this course, we are using Tidyverse packages dplyr, tidyr, and readr and installing them individually instead of the entire Tidyverse suite of packages. You can learn more about the Tidyverse and the packages it includes here: https://www.tidyverse.org/.\n\n\n\n\nSay you have an operation/function you want to apply to multiple elements or objects. Instead of writing out the same code for each element, for loops allow you to write the code once and iterate it over some sequence of elements. For example if you want to print() each element of a string of numbers, the code would look like:\n\nfor (i in 1:5) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\nYou can also apply for loops across character elements like this:\n\ncolors <- list(\"red\", \"blue\", \"green\")\n\nfor (i in colors) {\n  print(i)\n}\n\n[1] \"red\"\n[1] \"blue\"\n[1] \"green\"\n\n\nYou can replace print() with any other function or sequence of operations. It is also common to want to store the outputs of each for loop iteration. To do this you need to first create an empty list to feed each output into. You can create an empty list with list(), however with for loops it is better practice to specify the size of your list. For example to create an empty list of 3 elements you would run:\n\ntemp <- vector(\"list\", length = 3)\n\nprint(temp)\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n\nTo index elements of a list we use double brackets [[x]] .\nSo lets say we want to get the average (using the mean() function) of three different sets of numbers, and save each average value. We can feed the output of our for loop into our empty list temp like this:\n\n# our list of numbers\nnum <- list(c(1,2,3), c(5,6,7,8), c(9, 10, 11, 12))\nprint(num)\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 5 6 7 8\n\n[[3]]\n[1]  9 10 11 12\n\n\n\nfor (i in seq_along(num)) {\n  \n  temp[[i]] <- mean(num[[i]])\n}\n\nprint(temp)\n\n[[1]]\n[1] 2\n\n[[2]]\n[1] 6.5\n\n[[3]]\n[1] 10.5"
  },
  {
    "objectID": "basics.html#beginner-lessonstutorials",
    "href": "basics.html#beginner-lessonstutorials",
    "title": "R Basics",
    "section": "Beginner Lessons/Tutorials",
    "text": "Beginner Lessons/Tutorials\nIf you are brand new to R (or even if you have some experience) please read/work through these beginner resources:\nHands on Programming with R by Garrett Grolemund: The Very Basics. This covers getting started with RStudio, executing code from the console, writing functions, and working with scripts.\nSpatial Data Science with applications in R by Edzer Pebesma & Roger Bivand: R Basics. This covers pipes (which we will use a lot in this workshop), data structures, and how to dissect the different parts and structure of spatial data in R.\nBoth of the above lessons cover beginner material that will be relevant for this workshop."
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "Lesson 1",
    "section": "",
    "text": "As a first step whenever you start a new project, workflow, analysis, etc., it is good practice to set up an R project. R Projects are RStudio’s way of bundling together all your files for a specific project, such as data, scripts, results, figures. Your project directory also becomes your working directory, so everything is self-contained and easily portable.\nYou can start an R project in an existing directory or in a new one. To create a project go to File -> New Project:\n\nLet’s create a new directory and call it ‘R-for-Geospatial’. You can make it a sub directory of any folder you wish.\n\nNow we are working in our R project. You can see the working directory printed at the top of your console is now our project directory, and in the ‘Files’ tab in RStudio you can see we have an .Rproj file, which will open up this R project in RStudio whenever you come back to it. For example close out of this R session, navigate to the project folder on your computer, and double-click the .Rproj file."
  },
  {
    "objectID": "day1.html#read-in-r-packages",
    "href": "day1.html#read-in-r-packages",
    "title": "Lesson 1",
    "section": "Read in R packages",
    "text": "Read in R packages\nNow we have started a fresh R session in our new R project, we need to read in the libraries needed to work through today’s lesson. You should have all packages installed after finishing the set-up instructions on the Getting Started page.\nIn the set-up lesson, you used the following function to check if a package is installed, if not install it, and then load that package into your session.\n\npackageLoad <-\n  function(x) {\n    for (i in 1:length(x)) {\n      if (!x[i] %in% installed.packages()) {\n        install.packages(x[i])\n      }\n      library(x[i], character.only = TRUE)\n    }\n  }\n\nWe will be using this function the rest of the workshop to read in a list of packages at the beginning of each lesson, so lets store it as its own R script that we can call in later with the source() function. Sourcing functions is good practice as it reduces repetitiveness of rewriting them every time you want to use it.\nNow let’s use it to load in our libraries needed for today. Assuming you already installed all of these, loading them should run pretty quick.\n\npackageLoad(c(\"rgbif\", \"tidycensus\", \"tigris\", \"sf\", \"terra\", \"dplyr\", \"tidyr\", \"readr\"))\n\n\n\n\nToday we are going to walk through how to import spatial data directly into R, clean it, and save it so we can work with it the rest of the workshop.\n\n\n\nWe will work through how most of these data sets were imported and cleaned today, but not all of them are covered. To see how all data sets were created you can check out the Pulling Data page."
  },
  {
    "objectID": "day1.html#spatial-data-formats",
    "href": "day1.html#spatial-data-formats",
    "title": "Lesson 1",
    "section": "Spatial Data Formats",
    "text": "Spatial Data Formats\nVector Data\n\nLocations (points)\n\nCoordinates, address, country, city\n\nShapes (lines or polygons)\n\nPolitical boundaries, roads, building footprints, water bodies\n\n\nRaster Data\n\nImages (matrix of cells organized by rows and columns)\n\nSatellite imagery, climate, landcover, elevation"
  },
  {
    "objectID": "day1.html#import-and-manipulate-spatial-data",
    "href": "day1.html#import-and-manipulate-spatial-data",
    "title": "Lesson 1",
    "section": "Import and manipulate spatial data",
    "text": "Import and manipulate spatial data\nNow we have our project set up, packages loaded, and a basic understanding of the spatial data types we will be using. Let’s get some data to work with! For the first part we are going to import spatial data into R using the tigris, tidycensus, and rgbif packages. The raster data has been pre-processed (see Pulling Data for how that was done in R) and we will read those objects in from file.\nFirst, lets create a new folder in our project directory called ‘data/’ where we will be saving all the data used for this workshop.\n\nVector Data\n\ntigris\nAll the data we are working with in this course is confined to the state of Colorado. Let’s start by pulling in political boundaries for Colorado counties with the tigris package.\n\n# download county shapefile for the state of Colorado\ncounties <- tigris::counties(state = \"CO\")\n\ntigris has a lot of other available data sets in addition to political boundaries. In this course we are going to work with additional line shape files (rivers and roads) and polygons (urban areas).\nDue to the density of rivers and roads in Colorado and therefore large file size, we are going to limit this download to just Larimer County.\n\nrivers <- linear_water(state = \"CO\", county = \"Larimer\")\n\nroads <- roads(state = \"CO\", county = \"Larimer\")\n\nOur object rivers actually includes all linear water features in the county, such as streams/rivers, braided streams, canals, ditches, artificial paths, and aqueducts.\ncounties is a spatial polygon data set. We are going to work with one more polygon data set which includes the boundaries of individual urban areas and clusters across Colorado. We can explore the urban_areas() function from the tigris package to get more details on how urban areas are defined.\n\n?urban_areas\n\nNow let’s download the data set. By default it imports urban areas for all of the U.S., but we just want areas within Colorado. We have to do some data wrangling to filter out just Colorado urban areas. The column “NAME10” has the city and state of each urban area. We can use the separate() function from the tidyr package to split the “NAME10” column into two separate columns for city and state, and then we can use the filter() function to subset just the urban areas in Colorado.\nWe are also using a new function here, the pipe %>% operator. This allows us to run a sequence of operations without having to create intermediate objects that take up unnecessary space in your environment. It can be interpreted as ‘and then’, taking the output of one function ‘and then’ running the next function using that output as the input. We will be using the pipe a lot throughout this workshop.\n\nurban <- urban_areas() %>% \n  tidyr::separate(col = NAME10, sep = \", \", into = c(\"city\", \"state\")) %>% \n  dplyr::filter(state == \"CO\")\n\n\n\ntidycensus\ntidycensus is an R package that allows users to access U.S. Census data, imported as “tidy” data designed to work with tidyverse packages. The tidyverse is a collection of R packages designed for data manipulation, exploration, and visualization that you are likely to use in every day data analysis. They all use the same design philosophy, grammar, and data structures. When you install the tidyverse, it installs all of these packages, and you can then load all of them in your R session with library(tidyverse). You can learn more about the tidyverse and the packages it includes here: https://www.tidyverse.org/.\nThe tidycensus package requires the use of an API key to download data. You can obtain an API key for free at http://api.census.gov/data/key_signup.html\nOnce you have the API key, you need to run the following line of code before downloading any census data:\n\ncensus_api_key(\"PASTE YOUR API KEY HERE\")\n\n\n\n\nNow let’s first investigate what census variables are available (there’s a lot). tidycesus has two major functions: get_decennial() to access the 2000, 2010, and 2020 decennial U.S. Census data, and get_acs() to access the 1-year and 5-year American Community Survey (ACS) data. For this workshop we are going to work with ACS data.\nWe can get the full list of available variables with the load_variables() function, and we are interested in the 5-year ACS data for 2019 (the most recent year available). We then pass this output to View() which will open the data frame in a separate tab to be viewed similar to an excel spreadsheet.\n\nload_variables(2019, \"acs5\") %>% View()\n\nWe are going to download total population and median household income. We use the get_acs() function and specify we want this data at the county level for the state of Colorado and supply a string of the variables we want, using the variable ID.\n\ncensus <- get_acs(geography = \"county\", state = \"CO\", year = 2019,\n                  variables = c(\"B01003_001\", \"B19013_001\"), output = \"wide\")\n\nLet’s clean this up a bit. “E” in the variable name stands for estimate and “M” is margin of error. For our purposes we are only looking at the estimate values, so lets remove the other columns and rename the ones we keep with more informative titles.\n\ncensus <- census %>% \n  dplyr::select(contains(\"E\")) %>% \n  rename(total_pop = B01003_001E, med_income = B19013_001E)\n\n\n\nrgbif\nThe last of our spatial data download is species occurrences in the form of point data (latitude/longitude). rgbif is a package that allows you to download species occurrences from the Global Biodiversity Information Facility (GBIF), a database of global species occurrences with over 2.2 billion records.\nWe are going to pull occurrence data for a couple of charismatic Colorado species:\n\n\n\n\n\n\n\n\n\n\n\n\n\nElk\nYellow-Bellied Marmot\nWestern Tiger Salamander\n\n\n\nTo pull data for multiple species we are going to run the occ_data() function from the rgbif package over multiple species using a for loop. For a refresher on for loops see the R Basics page.\nWe first need to create a string of species scientific names to use in the download function, and a second string with their associated common names.\n\n#make a string of species names to use in the 'occ_data' function\nspecies <- c(\"Cervus canadensis\", \"Marmota flaviventris\", \"Ambystoma mavortium\")\n\n#also make a string of common names\ncommon_name <- c(\"Elk\", \"Yellow-bellied Marmot\", \"Western Tiger Salamander\")\n\nNow we are going to write a for loop that iterates across our three species. We first create an empty list that is the length of our species vector that we will fill with each species data download output and bind all the data together outside of the loop.\n\nocc <- vector(\"list\", length = length(species)) \n\n\nfor(i in 1:length(occ)){\n  \n  occ[[i]] <-\n    occ_data(\n      scientificName = species[i],\n      hasCoordinate = TRUE,\n      geometry = st_bbox(counties),\n      limit = 2000\n    ) %>%\n    .$data #return just the data frame. The '.' symbolizes the previous function's output\n  \n  # add species name column as ID to use later\n  occ[[i]]$ID <- common_name[i]\n  \n  #clean by removing duplicate occurrences\n  occ[[i]] <-\n    occ[[i]] %>% distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) %>%\n    dplyr::select(Species = ID,\n           decimalLatitude,\n           decimalLongitude,\n           year,\n           month,\n           basisOfRecord) #only keep relevant variables\n  \n  \n  \n  print(i) # this prints each element once its finished so you can see the progress\n  \n}\n\n# Bind all data frames together\nocc <- bind_rows(occ)\n\n\n\n\nRaster Data\nThe raster files for this workshop have already been processed (see Pulling Data for how this was done). Click each button below to download the GeoTIFF files for elevation and land cover (plus the necessary land cover attribute file) and save them in your ‘data/’ folder within this project.\n\n\n\n\n\n\n Download Elevation Data\n\n\n\n\n\n\n\n\n\n Download Land Cover Data\n\n\n\n\n\n\n\n\n\n Download Land Cover Attribute Data\n\n\n\nOnce saved, we can read in raster files using the rast() function from the terra package.\n\nelevation <- terra::rast(\"data/elevation_1km.tif\")\n\n\nterra::plot(elevation)"
  },
  {
    "objectID": "day1.html#coordinate-reference-systems",
    "href": "day1.html#coordinate-reference-systems",
    "title": "Lesson 1",
    "section": "Coordinate Reference Systems",
    "text": "Coordinate Reference Systems\nProbably the most important part of working with spatial data is the coordinate reference system (CRS) that is used. In order to analyze and visualize spatial data, all objects must be in the exact same CRS.\nWe can check a spatial object’s CRS by printing it to the console, which will print a bunch of metadata about the object.\n\nelevation\n\nclass       : SpatRaster \ndimensions  : 1322, 1724, 1  (nrow, ncol, nlyr)\nresolution  : 0.004895063, 0.004895063  (x, y)\nextent      : -109.6875, -101.2484, 36.59761, 43.06889  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat NAD83 (EPSG:4269) \nsource      : elevation_1km.tif \nname        : elevation_1km \nmin value   :           726 \nmax value   :          4288 \n\n\nBefore we start performing spatial analyses in Lesson 2 we will need to return to the CRS of these spatial objects and make sure they all match. If you need to transform a spatial object to a different CRS you can use the st_transform() function from the sf package for vector data, and project() from the terra package for raster data."
  },
  {
    "objectID": "day1.html#saving-spatial-data",
    "href": "day1.html#saving-spatial-data",
    "title": "Lesson 1",
    "section": "Saving spatial data",
    "text": "Saving spatial data\nYou do not necessarily need to run this part, since the ‘data/’ folder we downloaded at the beginning of today’s lesson contains all these data files we will need for tomorrow. However learning how to write spatial data to file is an important skill.\nTo save shapefiles, you can use the st_write() function from the sf package. If the intended file happens to already exists and you want to overwrite it, you would add append = FALSE to the argument.\n\nst_write(rivers, \"data/rivers.shp\")\n\nst_write(roads, \"data/roads.shp\")\n\nst_write(urban, \"data/urban_areas.shp\")\n\nst_write(counties, \"data/CO_counties.shp\")\n\nWe can save point files as a csv with the write_csv() function. We will go over tomorrow how to read a csv file with coordinates and convert it into a spatial object.\n\nwrite_csv(census, \"data/census_data.csv\")\n\nwrite_csv(occ, \"data/species_occ.csv\")"
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "Lesson 2",
    "section": "",
    "text": "In Lesson 1 you were exposed to spatial data types and various databases you can pull spatial data from. Today we are going to use those data sets to perform a range of spatial analyses.\nWe briefly used the sf and terra packages yesterday, but today we will be exploring them much more in depth using a range of spatial operations they provide.\nWe have to start by reading in the packages we need for today. Some are repeats from Lesson 1, but we also have a couple new ones. Namely ggplot2 and tmap to do some quick visualizations of our spatial outputs. In Lesson 3 we will be using these packages for more advanced data visualizations.\n\nsource(\"packageLoad.R\")\n\npackageLoad(c(\"dplyr\",\n              \"readr\",\n              \"sf\",\n              \"terra\",\n              \"tmap\",\n              \"ggplot2\"))\n\n\n\nWe’re going to start off today with some distance calculations. Using our species occurrence data, say we want to know which species is often found closest to rivers and/or roads. We can answer this by finding each species average distance (across all occurrences) to our rivers and roads shapefiles.\nFirst we have to read in the data. Our occurrences were saved as a csv file with lat/long. We can convert a non-spatial object (like a csv file) to a spatial sf object using the st_as_sf() function, specifying the CRS and lat/long columns.\n\nocc <- read_csv(\"data/species_occ.csv\") %>% \n  st_as_sf(coords = c(\"decimalLongitude\", \"decimalLatitude\"), crs = 4326)\n\nThroughout today we are going to be mapping our spatial data to quickly inspect it and get a visual of the data’s extent and characteristics.\ntmap is a great R package for spatial data visualization. It allows for both static (“plot” mode) and interactive (“view” mode) mapping options, which you can set using the function tmap_mode() . For today we will be making quick interactive plots. Once you set the mode with tmap_mode(), every plot call to tmap after that produces a plot in that mode.\n\ntmap_mode(\"view\")\n\nQuick view of all our points, colored by species:\n\ntm_shape(occ) +\n  tm_symbols(col = \"Species\", size = 0.5)\n\n\n\n\n\n\nAnother way to make a quick map is with tmap’s qtm() function, which stands for “Quick Thematic Map”\n\nqtm(occ, symbols.col = \"Species\")\n\n\n\n\n\n\nNow, for each species we want to find their average distance to rivers and roads. This involves point to line distance calculations, which we can perform with the sf package.\nFirst let’s read in our rivers and roads shapefiles. You can read in shapefiles with the st_read() function from sf, specifying the ‘.shp’ extension, which will work as long as all other accompanying spatial files (.shx, .dpf, .prj) are within the same folder.\n\nrivers <- st_read(\"data/rivers.shp\")\n\nReading layer `rivers' from data source \n  `C:\\Users\\ccmothes\\Desktop\\R-for-Geospatial\\data\\rivers.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 11066 features and 5 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -106.1837 ymin: 40.25778 xmax: -104.9431 ymax: 40.99842\nGeodetic CRS:  NAD83\n\nroads <- st_read(\"data/roads.shp\")\n\nReading layer `roads' from data source \n  `C:\\Users\\ccmothes\\Desktop\\R-for-Geospatial\\data\\roads.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 17737 features and 4 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -106.1944 ymin: 40.25778 xmax: -104.9431 ymax: 40.99844\nGeodetic CRS:  NAD83\n\n\nBefore performing any spatial operations, all of our spatial objects must be in the same CRS. We can see our spatial objects’ CRS when we print the object to the console, or we can get the full CRS details with the st_crs() function.\n\nst_crs(rivers)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nst_crs(roads)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\nst_crs(occ)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nSo our line shapefiles are in NAD83 and the occurrences are in WGS84. It generally doesn’t matter which CRS you choose to use, but here we are going to transform our occurrences to NAD83 so we only have to transform one object instead of two. Here we use the st_transform() function, specifying we want the new CRS for our occurrences to be that of the rivers shapefile.\n\nocc <- st_transform(occ, crs = st_crs(rivers))\n\nAlso, our occurrence data set covers all of Colorado, but rivers and roads are only for Larimer County. We have to first filter our points to the extent of the rivers and roads objects. However, the extent of these is a square bounding box, not the exact boundary of Larimer County. We can subset Larimer County from our Colorado counties object, and use st_filter() to filter points the are found within the Larimer County polygon.\n\ncounties <- st_read(\"data/CO_counties.shp\")\n\nReading layer `CO_counties' from data source \n  `C:\\Users\\ccmothes\\Desktop\\R-for-Geospatial\\data\\CO_counties.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 64 features and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -109.0602 ymin: 36.99245 xmax: -102.0415 ymax: 41.00344\nGeodetic CRS:  NAD83\n\nocc_larimer <- st_filter(occ, counties[counties$NAME == \"Larimer\",])\n\nqtm(occ_larimer)\n\n\n\n\n\n\nGreat, now we just have species occurrences within Larimer County.\nNow for each point we want to calculate its distance to the nearest river and road. Let’s start with rivers and then do the same for roads. The most efficient way is to first find the nearest line feature for each point. We can do this with the st_nearest_feature() function.\nThis function returns the index values (row number) of the river feature in the rivers spatial data frame that is closest in distance to each point. Here we are saving these index values in a new column of our Larimer occurrences that we will use later to calculate distances.\n\nocc_larimer$nearest_river <- st_nearest_feature(occ_larimer, rivers)\n\nNow, for each point we can use the st_distance() function to calculate the distance to the nearest river feature, using the index value in our new “nearest_river” column. Adding by_element = TRUE is necessary to tell the function to perform the distance calculations by element (row), which we will fill into a new column “river_dist_m”.\n\nocc_larimer$river_dist_m <- st_distance(occ_larimer, rivers[occ_larimer$nearest_river,], by_element = TRUE)\n\nNotice that the new column is more than just a numeric class, but a “units” class, specifying that the values are in meters.\n\nstr(occ_larimer)\n\nsf [1,613 × 7] (S3: sf/tbl_df/tbl/data.frame)\n $ Species      : chr [1:1613] \"Elk\" \"Elk\" \"Elk\" \"Elk\" ...\n $ year         : num [1:1613] 2022 2022 2022 2022 2022 ...\n $ month        : num [1:1613] 1 1 1 1 2 3 3 3 3 3 ...\n $ basisOfRecord: chr [1:1613] \"HUMAN_OBSERVATION\" \"HUMAN_OBSERVATION\" \"HUMAN_OBSERVATION\" \"HUMAN_OBSERVATION\" ...\n $ geometry     :sfc_POINT of length 1613; first list element:  'XY' num [1:2] -105.5 40.4\n $ nearest_river: int [1:1613] 1487 6 1019 2850 8370 8741 1466 1431 1431 1449 ...\n $ river_dist_m : Units: [m] num [1:1613] 105.1 532.9 306.6 68.3 43.3 ...\n - attr(*, \"sf_column\")= chr \"geometry\"\n - attr(*, \"agr\")= Factor w/ 3 levels \"constant\",\"aggregate\",..: NA NA NA NA NA NA\n  ..- attr(*, \"names\")= chr [1:6] \"Species\" \"year\" \"month\" \"basisOfRecord\" ...\n\n\nCool, now we have the distance to the nearest river (in meters) for each individual species occurrence. Now say we want the average distance for each species. We can do some data wrangling to get these values using the dplyr() package. Using the pipe %>% operator again, we perform a chain of operations on the data frame. group_by() specifies that all following operations should be performed individually by a grouping variable, in this case we want to apply operations on each individual species. Next summarise() calculates a new column “river_dist” that is the average (per species) river distance, where we have to convert our ‘units’ column to numeric to perform the mean() function.\n\nocc_larimer %>% \n  group_by(Species) %>% \n  summarise(river_dist = (mean(as.numeric(river_dist_m))))\n\n\n\n  \n\n\n\nLet’s make a quick bar plot to visually compare. We can pipe this output into a ggplot() object, specifying which variables go on the x and y axes within aes() and then we want to fill the bar color by species. geom_col() returns a barplot where the heights of the bars represent values in the data (in our case species average distance to a river).\n\nocc_larimer %>% \n  group_by(Species) %>% \n  summarise(river_dist = (mean(as.numeric(river_dist_m)))) %>% \n  ggplot(aes(Species, river_dist, fill = Species)) +\n  geom_col()\n\n\n\n\nA thing to note about ggplot2 is that it uses + instead of %>% to add additional elements.\nLet’s mess around with this plot a little bit further to make it look more aesthetic.\n\nocc_larimer %>% \n  group_by(Species) %>% \n  summarise(river_dist = mean(as.numeric(river_dist_m))) %>% \n  ggplot(aes(Species, river_dist, fill = Species)) +\n  geom_col() +\n  labs(y = \"Average distance to nearest river (m)\") +\n  theme(legend.position = \"none\") #removes the legend\n\n\n\n\nNow lets do the same thing, but calculate average distance to the nearest road.\n\nocc_larimer$nearest_road <- st_nearest_feature(occ_larimer, roads)\n\nocc_larimer$road_dist_m <- st_distance(occ_larimer, roads[occ_larimer$nearest_road,], by_element = TRUE)\n\nocc_larimer %>% \n  group_by(Species) %>% \n  summarise(road_dist = mean(as.numeric(road_dist_m))) %>% \n\n  ggplot(aes(Species, road_dist, fill = Species)) +\n  geom_col() +\n  labs(y = \"Average distance to nearest road (m)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nAlternatively, say you want to know what percentage of species’ occurrences (points) were found within a certain distance of a river or a road (calculated buffer).\nTo do this we could add a buffer around our line features and filter the points that fall within that buffer zone. For this example let’s say we are interested in the 100 m buffer zone around rivers and roads. However, if you try this you’ll notice this operation takes quite a while.\n\nriver_buffer <- st_buffer(rivers, dist = 100)\n\nInstead, a more efficient way would be to make a 100 m buffer around each point, and see how many intersect with a river or road.\n\nocc_buffer <- st_buffer(occ_larimer, dist = 100)\n\nStill takes a little bit of run time, but much faster than buffering each line feature. Our occ_buffer object is now a spatial polygon data frame, where each feature is an occurrence buffer with 100 m radius.\n\n\n\nWe can conduct spatial intersect operations using the function st_intersects(). This function checks if each individual buffer intersects with a river, and if so it returns an index value (row number) for each river feature it intersects. This function returns a list object for each buffer polygon, that will be empty if there are no intersections. We will add this as a column to our buffer data set, and then create a binary yes/no river intersection column based on those results.\n\nriver_intersections <- st_intersects(occ_buffer, rivers)\n\nIf we inspect this object, we see it is a list of the same length as our occ_buffer object, where each list element is either empty (no intersections) or a list of index numbers for the river features that do intersect that buffer.\nWe want to create a new column that returns TRUE/FALSE if the buffer intersects with a river. We do this by testing if the length of each element is greater than 0 (if not the element is empty and returns FALSE since there are no river intersections).\n\nocc_buffer$river_100m <- lengths(river_intersections) > 0\n\nNow we can find out what percentage of occurrences are within 100 m of a river for each species using similar dplyr operations as before.\n\nocc_buffer %>% \n  st_drop_geometry() %>% #we use this function to treat the object as a dataframe\n  group_by(Species) %>% \n  summarise(total_occ = n(), percent_river = (sum(river_100m == TRUE)/total_occ)*100)\n\n\n\n  \n\n\n\nNow lets do another type of spatial intersection. Say we want to know what percent of each county is defined as ‘urban area’, using our urban areas polygons.\nLet’s read in our urban areas polygon shapefile.\n\nurban <- st_read(\"data/urban_areas.shp\")\n\nReading layer `urban_areas' from data source \n  `C:\\Users\\ccmothes\\Desktop\\R-for-Geospatial\\data\\urban_areas.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 64 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.7507 ymin: 37.14073 xmax: -102.2416 ymax: 40.75509\nGeodetic CRS:  NAD83\n\n\nNow since we are wanting to calculate the actual area of intersection (not just whether or not an intersection exists as before) we can use the st_intersection() function.\n\nurban_intersect <- st_intersection(counties, urban)\n\nThis function returns the urban areas polygons as new polygons that intersect within each county, tied to the county information it lies within. We see that there are more rows in this data set than the original counties data set, meaning some urban areas cross multiple counties.\nTo clean this data to get the results we’re interested in (percentage of each county that is covered by urban areas) we will first calculate the area of each urban area intersection, sum the total intersecting areas per county if there are multiple, and divide that sum by total county area. We are using a new function here from the dplyr package, mutate(), which creates new columns based on specified values/calculations.\n\nintersect_area <- urban_intersect %>% \n  mutate(intersect_area = st_area(.)) %>% #create a new column with shape area\n  dplyr::select(NAME, intersect_area) %>% #reduce to just county name and intersect area columns\n  group_by(NAME) %>% # group by county\n  summarise(intersect_area = sum(intersect_area)) %>% #new column that sums intersect area per county\n  st_drop_geometry() #drop the geometry to treat as a dataframe\n\nWe return this as a data frame since we just want the intersection area values per county. We can then join this data frame to our counties shapefile to calculate percent urban area coverage. We are going to create a new counties object called counties_attr that we will be adding a lot of other county-level attributes to. We are using another new function here left_join(), which is a member of dplyr’s join functions where in this case the output contains all rows in x, the first element, and joins y, the second element, by a matching column name/variable, in this case county name.\n\ncounties_attr <- counties %>%\n  mutate(county_area = st_area(.)) %>% #calculate county area\n  left_join(intersect_area, by = \"NAME\") %>% #join county to intersect area data\n  mutate(urban_coverage = as.numeric(intersect_area / county_area)) # calculate a new column that is the proportion of urban area coverage\n\nNow let’s visualize urban coverage by county\n\nqtm(counties_attr, fill = \"urban_coverage\")\n\n\n\n\n\n\nWe can also look at total urban area instead of a percentage.\n\nqtm(counties_attr, fill = \"intersect_area\")\n\n\n\n\n\n\nA little more variation here but maybe nothing too exciting if you are familiar with Colorado metro areas. But at least you learned some cool new tools related to spatial intersections!\n\n\n\nWe already used this tool a little already, but we can use left_join() to add a bunch more attributes to our counties shapefile. First lets add our census data, which was collected at the county level and saved as a csv.\n\ncensus <- read_csv(\"data/census_data.csv\") %>% \n  dplyr::select(-NAME)\n\nWe first want to remove the ‘NAME’ column, because it is slightly different than the ‘NAME’ column in our counties data and therefore will not join properly. We can instead join by matching ‘GEOID’, which is a unique numeric ID given to each county.\n\ncounties_attr <- counties_attr %>% \n  left_join(census, by = \"GEOID\")\n\nLastly, lets join our species occurrence data set. Say we want to know how many species occurrences are found in each county. Here we go back to the st_intersects() function we used before, which returns a list of all spatial elements that intersect with the spatial features of interest, in this case how many occurrence points intersect with each county polygon. We then nest this within lengths() which will return the number of intersecting features (occurrences) for each county, and we put those values as a new column called “species_count”.\n\ncounties_attr$species_count <- lengths(st_intersects(counties_attr, occ))\n\nNow we have a bunch of information tied to our county shapefile, we can explore it spatially and comparatively.\n\nqtm(counties_attr, fill = \"species_count\")\n\n\n\n\n\n\nUse ggplot2 to visualize the relationship between different county attributes.\n\ncounties_attr %>% \n  st_drop_geometry() %>% \n  ggplot(aes(total_pop, species_count)) +\n  geom_point() +\n  stat_smooth()\n\n\n\n\n\n# urban area related to co_born\ncounties_attr %>% \n  st_drop_geometry() %>% \n  ggplot(aes(urban_coverage, species_count)) +\n  geom_point() +\n  stat_smooth()\n\n\n\n\n\n\n\nSo far we’ve dealt with a bunch of vector data and associated analyses with the sf package. Now lets work through some raster data analysis using the terra package.\nLets read in our land cover and elevation raster files. These are files you downloaded yesterday, but were already processed. You can read more about these data sets and how they were processed here. Land cover data comes from the National Land Cover Database (NLCD) and elevation data comes from the AWS Open Data Terrain Tiles. You can read in a raster file with the rast() function from terra.\n\nlandcover <- terra::rast(\"data/NLCD_CO.tif\")\n\nelevation <- terra::rast(\"data/elevation_1km.tif\")\n\n\nqtm(landcover)\n\n\n\n\n\nThis land cover data set includes attributes (land cover classes) associated with raster values. We can quickly view the frequency of each land cover type with the freq() function.\n\nfreq(landcover)\n\n\n\n  \n\n\n\nLet’s view this as a bar chart.\n\nfreq(landcover) %>% \n  ggplot(aes(reorder(value, count), count)) + \n  labs(x = \"\") +\n  geom_col() +\n  coord_flip() # switch the axes to better view land cover class names\n\n\n\n\nSay we want to explore some habitat characteristics of our species of interest, and we are specifically interested in forest cover. Our first step is to create a new raster layer from our land cover layer representing percent forest cover. This will involve multiple operations, including raster reclassification and focal statistics. Specifically, say we want to calculate the average percentage of forest cover and urbanization within a 9x9 pixel moving window.\nFirst lets reclassify our land cover raster, creating a new raster representing just forest/non-forest pixels.\nSince rasters are technically matrices, we can index and change values using matrix operations. Given this particular raster has attributes (land cover class names) associated with values, we can index by those names.\n\n#first assign landcover to a new object name so we can manipulate it while keeping the origian\nforest <- landcover\n\n#where the raster equals any of the forest categories, set that value to 1\nforest[forest %in% c(\"Deciduous Forest\", \"Evergreen Forest\", \"Mixed Forest\")] <- 1\n\n#SPELLING IS IMPORTANT\n\n#now set all non forest pixels to NA\nforest[forest != 1] <- NA\n\nLets look at our new forest layer\n\nplot(forest)\n\n\n\n\n\n\n\nNow we are going to perform focal statistics, which is a spatial operation that calculates new values for each cell based on a specified moving window. For this example we are going to calculate within a 9x9km moving window (since our pixel resolution is 1km). We supply this to the w = argument as a matrix, where the first value is the weight of each pixel, and the second two are the number of rows and columns. Second we use the “sum” function, since each forest pixel has a value of 1 we will get the total number of forest pixels within the moving window, and then later divide the values by the total number of pixels in the window (81) to get the percentage. The final raster values will represent for each pixel the surrounding forest percentage (within ~4.5 km radius).\nNote: We are just making up these distance numbers to demonstrate the use of terra functions. In reality when working on your own research questions you should spend time thinking about the most appropriate values to use for your study species/system.\n\nforest_pct <- terra::focal(forest, w=matrix(1,9,9), fun = \"sum\", na.rm = TRUE)\n\n\nforest_pct <- forest_pct/81\n\n\nplot(forest_pct)\n\n\n\n\nNext, we wanted to know the percent forest cover associated with each species occurrence. Since we are now working with multiple spatial objects, we have to first check they are all in the same CRS and if not transform the data before any spatial operations.\n\ncrs(forest_pct)\n\n[1] \"PROJCRS[\\\"Albers Conical Equal Area\\\",\\n    BASEGEOGCRS[\\\"WGS 84\\\",\\n        DATUM[\\\"World Geodetic System 1984\\\",\\n            ELLIPSOID[\\\"WGS 84\\\",6378137,298.257223563,\\n                LENGTHUNIT[\\\"metre\\\",1]]],\\n        PRIMEM[\\\"Greenwich\\\",0,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433]],\\n        ID[\\\"EPSG\\\",4326]],\\n    CONVERSION[\\\"Albers Equal Area\\\",\\n        METHOD[\\\"Albers Equal Area\\\",\\n            ID[\\\"EPSG\\\",9822]],\\n        PARAMETER[\\\"Latitude of false origin\\\",23,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8821]],\\n        PARAMETER[\\\"Longitude of false origin\\\",-96,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8822]],\\n        PARAMETER[\\\"Latitude of 1st standard parallel\\\",29.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8823]],\\n        PARAMETER[\\\"Latitude of 2nd standard parallel\\\",45.5,\\n            ANGLEUNIT[\\\"degree\\\",0.0174532925199433],\\n            ID[\\\"EPSG\\\",8824]],\\n        PARAMETER[\\\"Easting at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8826]],\\n        PARAMETER[\\\"Northing at false origin\\\",0,\\n            LENGTHUNIT[\\\"metre\\\",1],\\n            ID[\\\"EPSG\\\",8827]]],\\n    CS[Cartesian,2],\\n        AXIS[\\\"easting\\\",east,\\n            ORDER[1],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]],\\n        AXIS[\\\"northing\\\",north,\\n            ORDER[2],\\n            LENGTHUNIT[\\\"metre\\\",1,\\n                ID[\\\"EPSG\\\",9001]]]]\"\n\nst_crs(occ)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nLooks like the raster layer is in a different CRS. Let’s reproject this so we can use it with our vector data (which are all in NAD83). We can project raster data to a new CRS with the project() function from terra.\nOne thing to note is that while terra does work with vector data, it wants them to be in a special format called a SpatVector , instead of an sf object. Luckily they have made it quick and easy to convert between the data formats using the vect() function, so we just need to remember to nest our sf objects within that function when using them in terra functions.\n\nforest_pct <- project(forest_pct, vect(occ))\n\n\n\n\nNow we can use the extract() function to extract the raster pixel value at each occurrence.\n\nterra::extract(forest_pct, vect(occ))\n\n\n\n  \n\n\n\nNotice that this returns a 2 column data frame, with an ID for each feature (occurrence) and the extracted raster value in the second column. We want to add these raster values as a new column to our occurrence data set, so we need to index just this second column of the extract() output.\n\nocc$forest_pct <- terra::extract(forest_pct, vect(occ))[,2]\n\nNow let’s do the same extract method with our elevation raster, pulling the elevation value at each species occurrence.\nAgain, we need to first project the raster to the CRS of the occurrence data set.\n\nelevation <-  terra::project(elevation, vect(occ))\n\nNow we can do the same extraction as with the forest raster, putting the values in a new ‘elevation’ column\n\nocc$elevation <- terra::extract(elevation, vect(occ))[,2]\n\nLet’s do some data frame operations to calculate and compare average forest cover across species\n\nocc %>% \n  group_by(Species) %>% \n  summarise(avg_forest_pct = mean(forest_pct, na.rm = TRUE))\n\n\n\n  \n\n\n\n\nocc %>% \n  group_by(Species) %>% \n  summarise(avg_forest_pct = mean(forest_pct, na.rm = TRUE)) %>% \n  ggplot(aes(Species, avg_forest_pct, fill = Species))+\n  geom_col() +\n  theme(legend.position = \"bottom\")\n\n\n\n\nLooks like elk are associated with the most forested habitats. We can also view the spread of values with a box plot.\n\nggplot(occ, aes(Species, forest_pct)) +\n  geom_boxplot()\n\n\n\n\nHow about elevation?\n\nggplot(occ, aes(Species, elevation)) +\n  geom_boxplot()\n\n\n\n\nYellow-bellied marmots are found at the highest elevations, on average.\nThat’s one way to use the extract() function. We can also extract raster values within polygons, and supply a function to summarize those raster values.\nSay we wanted to know the most common land cover type in each county. We can use extract() with the function ‘modal’ to return the most common type for each polygon, and we will add this as a new column.\nAgain we need to project our land cover raster.\n\nlandcover_prj <- project(landcover, vect(counties))\n\n\nterra::extract(landcover_prj, vect(counties), fun = \"modal\")\n\n\n\n  \n\n\n\nThis works similar to what we ran with the occurrence dataset…however we notice that it returns the raw raster values instead of the named land cover classes (which we want). This raster behaves such that values are named factors (as long as we have the land cover attribute file in the same folder), and we can pull this metadata with the cats() function.\n\ncats(landcover)[[1]]\n\n\n\n  \n\n\n\nThis returns a single element list that is a data frame with a bunch of information. We just want to keep ‘value’ and ’NLCD Land Cover Class”, and remove all the empty classes. We have to first index the first element of the list to operate on just the data frame, then we can apply some dplyr functions.\n\nnlcd_classes <- cats(landcover)[[1]] %>% \n  dplyr::select(\"value\", nlcd_class = \"NLCD Land Cover Class\") %>% \n  filter(nlcd_class != \"\")\n\nCool, now we can tie this to our counties data frame once we have the most common land cover value calculated with extract\n\ncounties$common_landcover <- terra::extract(landcover_prj, vect(counties), fun = \"modal\")[,2]\n\nThis join is different from our others because the variable we want to join by has a different column name in each data set (even though the values match), but we can specify this within the by = argument like this:\n\ncounties <- counties %>% \n  left_join(nlcd_classes, by = c(\"common_landcover\" = \"value\"))\n\nWe can now get some summary statistics on the most common land cover types per county.\n\ncounties %>% \n  st_drop_geometry() %>% #for ggplot, don't need geometry\n  group_by(nlcd_class) %>% \n  summarise(n = n()) %>% \n  ggplot(aes(nlcd_class, n, fill = nlcd_class)) +\n  geom_col()+\n  theme(legend.position = \"none\")+\n  coord_flip()\n\n\n\n\nAnd map it out by county:\n\ntm_shape(counties)+\n  tm_polygons(\"nlcd_class\")\n\n\n\n\n\n\n\n\n\nWe’ve added new columns to some of our data frames and created new ones. Let’s save these objects so we can use them in Lesson 3 without having to re-run all this code.\nYesterday we learned how to save shapefiles with st_write(). Another way to save data in R is by saving them as R objects, which is beneficial as it reduces file size. You can save individual R objects as .RDS files, or multiple R objects as .RData files. Since we have multiple objects we want to save and load back into our environment tomorrow, we will write them to a single .RData file.\nBefore writing to file, let’s get rid of some of the extra columns we don’t need. counties_attr is one of the main objects we will be using for visualizations tomorrow. Here we can de-select a range of columns by putting a - in front.\n\ncounties_attr <- counties_attr %>% \n  dplyr::select(-c(NAMELSAD:INTPTLON))\n\nNow we use the save() function, listing all the objects in our current environment that we want to save, and the file name and location with the .RData extension.\n\nsave(counties_attr, occ_buffer, occ_larimer, occ, file = \"data/objects.RData\")\n\n\n\n\nWe will start Lesson 3 by reading these objects back into our environment (so you can close out of this R Studio session at the end of the day if you want)."
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "Lesson 3",
    "section": "",
    "text": "We did some basic plotting in Lesson 2 to view the results of our spatial analyses. In this lesson we will be working with some more advanced mapping techniques, plotting multiple spatial layers together, and learn how to make these interactive along with ways to share data and visualizations with others.\nFirst we need to call in the packages needed for today’s lesson. Load in our packageLoad() function that we have now saved as a script with source() and read in the following packages:\n\nsource(\"packageLoad.R\")\n\npackageLoad(c(\"tmap\", \"ggplot2\", \"shiny\", \"rmarkdown\", \"dplyr\", \"terra\", \"sf\"))\n\nNext we need to read in data from Lesson 2. Since we saved these as R objects in an .RData format, we can load those objects back into the session with load().\nIf for any reason you were not able to save the objects from Lesson 2, you can download the .RData file here:\n\n\n\n\n\n\n Download .RData file\n\n\n\nNote: If we saved a .RData file in the main project directory, these objects would load in the environment every time you open a new session. This is related to the question you may get every time you close out of R Studio that says “Do you want to save your workspace”. It is best practice to always say no (you can set this in your global options), otherwise you will be saving your entire R environment every time which likely consists of too much extra data you don’t need. Here we are saving the objects we know we will use again and that had required quite a bit of code to create them.\n\nload(\"data/objects.RData\") #path to the .RData file\n\n\n\n\n\n\nLet’s start visually exploring our counties data. R has a base plot() function which we used briefly in previous lessons. Since counties_attr is a spatial data frame, we have to specify the “geometry” column to plot() to make it spatial.\n\nplot(counties_attr$geometry)\n\n\n\n\nWe used gpglot2 in Lesson 2 to make some bar and line charts, but this package also has the capability of mapping spatial data, specifically sf objects, with the geom_sf() function:\n\nggplot(data = counties_attr) +\n  geom_sf()\n\n\n\n\nSay we want to color counties by our total population variable:\n\nggplot(data = counties_attr, aes(fill = total_pop)) +\n  geom_sf()\n\n\n\n\ngeom_sf() interprets the geometry of the sf object and visualizes it with the ‘fill’ value given.\n\n\nHere are some ways to make a more publication ready map:\n\nggplot(data = counties_attr, aes(fill = total_pop)) +\n  geom_sf() +\n  scale_fill_distiller(palette = \"OrRd\", direction = 1) +\n  labs(title = \"Total Population by Colorado County, 2019\",\n       fill = \"Total Population\",\n       caption = \"Data source: 2019 5-year ACS, US Census Bureau\") +\n  theme_void()\n\n\n\n\nYou can save ggplot2 maps/plots either directly from the “Plots” viewing pane or with the ggsave() function, which allows for a little more customization in your figure output.\n\n?ggsave\n\n\n\n\n\nWe’ve already been using the qtm() function with tmap to quickly view our results, but there are also a lot of ways to create custom cartographic products with this package.\nSet tmap_mode() to “plot” to make static maps, in case you were still set to interactive mode from Lesson 2.\n\ntmap_mode(\"plot\")\n\nThe general structure of tmap maps is to first initialize the map with tm_shape supplied with the spatial object, and then the following function depends on what geometry or symbology you want. We are going to first map just our county polygons so will use the tm_polygons() function.\n\ntm_shape(counties_attr) +\n  tm_polygons()\n\n\n\n\nWe can color polygons by a variable using the col = argument:\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\")\n\n\n\n\nA difference we see between our tmap and ggplot2 maps is that by default tmap uses a classified color scheme rather than a continuous once. By default tmap sets the classification based on the data range, here choosing intervals of 200,000.\nGiven this classified structure, say you also wanted to see the distribution of the raw values:\n\nhist(counties_attr$total_pop)\n\n\n\n\nWe can manually change the classification of our map within the tm_polygons() function with the style = argument. Let’s try using a quantile method, where each class contains the same number of counties. tm_layout() also offers a lot of options to customize the map layout. Here we remove the map frame and put the legend outside the map area.\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\",\n              style = \"quantile\",\n              n = 5,\n              title = \"Total Population by County\")+\n  tm_layout(frame = FALSE,\n            legend.outside = TRUE)\n\n\n\n\nBased on the quantile classification, we can see a little more heterogeneity now. We can even add our histogram of the data distribution to the plot too with legend.hist = TRUE.\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\",\n              style = \"quantile\",\n              n = 5,\n              title = \"Total Population by County\",\n              legend.hist = TRUE)+\n  tm_layout(frame = FALSE,\n            legend.outside = TRUE,\n            legend.hist.width = 5)\n\n\n\n\ntmap also has functions to add more customization like a compass, scale bar and map credits.\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\",\n              style = \"quantile\",\n              n = 5,\n              title = \"Total Population by County\",\n              legend.hist = TRUE)+\n  tm_layout(frame = FALSE,\n            legend.outside = TRUE,\n            legend.hist.width = 5) +\n  tm_scale_bar(position = c(\"left\", \"bottom\")) +\n  tm_compass(position = c(\"right\", \"top\")) +\n  tm_credits(\"Map credit goes here\", position = c(\"right\", \"bottom\"))\n\n\n\n\nYou can save your maps with the tmap_save() function\n\n?tmap_save\n\nWe can also view attributes as graduated symbols with tm_bubbles()\n\ntm_shape(counties_attr) +\n  tm_polygons() +  # add base county boundaries\n  tm_bubbles(size = \"total_pop\",\n             col = \"red\",\n             alpha = 0.5) +\n  tm_layout(legend.outside = TRUE,\n            legend.outside.position = \"bottom\")\n\n\n\n\nBuilding off of this, we can view multiple attributes at once using polygon colors and graduated symbols. Say we want to color county by total population and add graduated symbols for total species occurrences per county.\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\",\n              style = \"quantile\", n = 5,\n              title = \"Total Population\") +\n  tm_bubbles(size = \"species_count\",\n             col = \"navy\",\n             alpha = 0.5,\n             title.size = \"Species Occurrences\") +\n  tm_layout(frame = FALSE,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\")\n\n\n\n\nYou can also add layers from multiple sf objects by calling a new tm_shape:\n\ntm_shape(counties_attr) +\n  tm_polygons(col = \"total_pop\",\n              style = \"quantile\",\n              palette = \"Greys\",\n               n = 5,\n              title = \"Total Population\") +\ntm_shape(occ) +\n  tm_symbols(col = \"Species\",\n             palette = \"Dark2\",\n             alpha = 0.8,\n             size = 0.5) +\n  tm_layout(frame = FALSE,\n            legend.outside = TRUE,\n            legend.outside.position = \"right\")\n\n\n\n\n\n\nCan’t decide on a color palette? tmap has a built in tool that allows you decide.\n\ntmaptools::palette_explorer()\n\n\nWant a cool tmap tip?\n\ntmap_tip()\n\nSearch for multiple locations directly with qtm. It uses tmaptools::geocode_OSM. \nNew since tmap 3.1 \n\nqtm(c(\"Paris\", \"Amsterdam\", \"Berlin\", \"London\")) \n\n\n\n\n\nWant to compare across multiple variables? We can quickly do that with tm_facets() or by supplying a string of column names within tm_polygons, depending on the format of your data.\nLets first compare across our census variables, which are organized as different columns in counties_attr\n\ntm_shape(counties_attr) +\n  tm_polygons(c(\"total_pop\", \"med_income\"),\n              style = \"quantile\", n = 5,\n              title = c(\"Total Population\", \"Median Income\"))+\n  tm_facets(ncol = 2) +\n  tm_layout(frame = FALSE)\n\n\n\n\nSecond, we can compare across values in one column by adding the by = argument to tm_facets(). Here let’s make an individual map for each species.\n\ntm_shape(counties_attr) +\n  tm_polygons() +\ntm_shape(occ) +\n  tm_facets(by = \"Species\", free.coords = FALSE) +\n  tm_symbols(col = \"Species\", palette = c(\"red\", \"yellow\", \"blue\"),\n             alpha = 0.5) +\n  tm_layout(legend.show = FALSE)\n\n\n\n\nWe can also make these facet maps interactive, and sync the zoom and scrolling across all facets with sync = TRUE\n\ntmap_mode(\"view\")\n\n\n\n\n\ntm_shape(counties_attr) +\n  tm_polygons(c(\"total_pop\", \"med_income\"),\n              style = \"quantile\", n = 5,\n              title = c(\"Total Population\", \"Median Income\"))+\n  tm_facets(ncol = 2, sync = TRUE) +\n  tm_layout(frame = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnnimations are a powerful (and fun!) visualization method when you have time series data. Our species occurrence data has year associated with it, so we could make an animation of observations over time.\nLet’s go back to static plot mode:\n\ntmap_mode(\"plot\")\n\nSince we have a lot of species data, for this example let’s look at just the Elk occurrences. Here we are using dplyr’s filter() function to filter rows that are just Elk observations and remove any that do not have year data.\n\nelk_occ <- occ %>% \n  dplyr::filter(Species == \"Elk\", !is.na(year))\n\nWe can make an animation with tmap_animation(). To do so we need to create a tmap object first, and must set the nrow and ncol to 1 within tm_facets(). We also set free.coords = FALSE which will keep the zoom level of the map constant across animation frames. We then supply this object and other animation settings to tmap_animation().\n\nm1 <- tm_shape(counties_attr) +\n  tm_polygons() +\n  tm_shape(elk_occ) +\n  tm_symbols(col = \"red\", alpha = 0.8) +\n  tm_facets(along = \"year\", free.coords = FALSE, nrow = 1, ncol = 1)\n\n\ntmap_animation(m1, filename = \"data/elk_occ.gif\", width = 1200, height = 600, delay = 80)\n\n\n\n\n\nLet’s go back to interactive mode and walk through how to further use and customize interactive maps.\n\ntmap_mode(\"view\")\n\nTo learn the ins and outs of interactive mapping, we are going to make a map with three layers: our elevation raster, urban areas polygons, and species occurrences.\nWe already have our occurrence data set loaded, lets read in our elevation and urban areas files. For efficiency, we can use the %>% operator to process our elevation raster in a single step, which includes projecting it to the CRS of our occurrence data and cropping it to the occurrence extent (the raw file extended a little outside of the Colorado boundary).\n\nurban <- st_read(\"data/urban_areas.shp\")\n\nReading layer `urban_areas' from data source \n  `C:\\Users\\ccmothes\\Desktop\\R-for-Geospatial\\data\\urban_areas.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 64 features and 13 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -108.7507 ymin: 37.14073 xmax: -102.2416 ymax: 40.75509\nGeodetic CRS:  NAD83\n\nelevation <- terra::rast(\"data/elevation_1km.tif\") %>% \n  terra::project(vect(occ)) %>% \n  terra::crop(vect(occ))\n\nNow lets add all of them to our interactive map. Note that alpha controls the transparency/opacity of layers, with a range of 0 (totally transparent) to 1 (non-transparent).\n\ntm_shape(occ) +\n  tm_dots(col = \"Species\",\n             size = 0.1,\n             palette = \"Dark2\",\n             title = \"Species Occurrences\") +\ntm_shape(urban) +\n  tm_polygons(alpha = 0.7, title = \"Urban Areas\") +\ntm_shape(elevation) +\n  tm_raster(alpha = 0.8, title = \"Elevation (m)\")\n\n\n\n\n\n\nTo improve the user experience, we can customize what content displays in the pop-up windows. Let’s add some information associated with each species’ occurrence.\n\ntm_shape(occ) +\n  tm_dots(\n    col = \"Species\",\n    size = 0.1,\n    palette = \"Dark2\",\n    title = \"Species Occurrences\",\n    popup.vars = c(\"Record Type\" = \"basisOfRecord\",\n                   \"Year\" = \"year\",\n                   \"Month\" = \"month\",\n                   \"Elevation (m)\" = \"elevation\")\n  ) +\n  tm_shape(urban) +\n  tm_polygons(alpha = 0.7, title = \"Urban Areas\") +\n  tm_shape(elevation) +\n  tm_raster(alpha = 0.8, title = \"Elevation (m)\")\n\n\n\n\n\n\n\n\n\nSo far we have used ggplot2 and tmap extensively. It is important to note there are many other spatial data visualization packages, but we wanted to reduce the amount of package installation required for this workshop. tmap is unique because of its breadth of functionality, like static and interactive mapping, animations, etc. Others worth investigating are mapview , leaflet and plotly for interactive visualizations.\n\n\n\n\n\nR markdown is a fantastic notebook-style interface that combines text and code to produce reproducible analyses and workflows and generate high quality reports that can be shared with an audience. You should have installed the rmarkdown package in the set-up stage of this workshop. We are going to run through a quick example of how to use R Markdown. Start by going to File -> New File -> R Markdown.\nPut in your details like title and author, then investigate the draft document it creates for you. If you hit the knit button at the top of your document, it should ask where you want to save the file, and then it renders a HTML document like this:\n\nYou can then share these documents with your intended audience, or host them on the web with (free) publishing services such as RPubs.\n\n\n\nShiny is an R package that takes interactivity to another level through interactive web applications, allowing users to interact with any aspect of your data and analysis. You can host them as standalone web apps or embed them within R Markdown documents or build dashboards. And the best part is…you can do it all within R, no web development skills required!\nSo, lets build a quick shiny app! Rolling with the multi-layer interactive map we just made above, let’s make an app that allows users to interact with the data. For example, we have a lot of species occurrence data. Based on various attributes, we could allow users to choose what they want to see on the map, such as which species, what year they were observed, and what elevation they were found at.\nShiny apps are contained in a single script called app.R . app.R has three components:\n\na user interface (ui) object, which controls the layout and appearance of your app\na server function, which contains the instructions needed to build your app\na call to shinyApp() which creates your web application based on your ui/server objects.\n\nLet’s create a new shiny app by going to File -> New File -> Shiny Web App\nThis creates an outline of our shiny app, with the ui and server objects and a call to shinyApp() at the end. Since shiny apps are self contained within the app.R file, at the top of the file define which libraries you need and read in your data. app.R files assume the working directory is the directory the app.R file lives in, so for this example save it to the root project directory.\nFirst let’s define the UI. Our layout is going to be a fluid page with a title panel, followed by a sidebar layout with a main panel (our map) and a side panel (user inputs). You can learn more about the different layout types here.\n\nui <- fluidPage(\n  \n  #App title\n  titlePanel(\"Species of Colorado\"),\n  \n  # Add some informational text\n  h5(\n    \"This map shows occurrence data for multiple Colorado species in relationship to elevation and urban areas.\"\n  ),\n  h5(\"In this app you can filter occurrences by species, year of observation, and elevation. You can also click on individual occurrences to view metadata.\"),\n  \n  # Sidebar layout\n  sidebarLayout(\n    \n    # Sidebar panel for widgets that users can interact with\n    sidebarPanel(\n    \n    # Input: select species shown on map\n    checkboxGroupInput(\n      inputId = \"species\",\n      label = \"Species\",\n      choices = list(\n        \"Elk\", \"Yellow-bellied Marmot\", \"Western Tiger Salamander\"\n      ),\n      selected = c(\"Elk\", \"Yellow-bellied Marmot\", \"Western Tiger Salamander\")\n    ),\n    \n    # Input: Filter points by year observed\n    sliderInput(inputId = \"year\", label = \"Year\",\n                min = 1800, max = 2022, value = c(1800,2022), sep=\"\"),\n    \n    \n    # Input: Filter by elevation\n    sliderInput(inputId = \"elevation\",\n                label = \"Elevation\",\n                min = 1000, max = 4500, value = c(1000,4500))\n    \n  ),\n  \n  # Main panel for displaying output (our map)\n  mainPanel(\n    \n    # Output: interactive map\n    tmapOutput(\"map\")\n  )\n  \n)\n  \n)\n\nNow define the server logic that draws the map based on user inputs\n\nserver <- function(input, output){\n  \n  # Make a reactive object, meaning an object that will change based on user input\n  occ_react <- reactive(\n    occ %>% \n      filter(Species %in% input$species) %>% \n      filter(year >= input$year[1] & year <= input$year[2]) %>% \n      filter(elevation >= input$elevation[1] & \n             elevation <= input$elevation[2])\n  )\n  \n  # Render the map based on our reactive occurrence dataset\n  output$map <- renderTmap({\n    \n    tmap_mode(\"view\")\n    \n    tm_shape(occ_react()) +\n      tm_dots(\n        col = \"Species\",\n        size = 0.1,\n        palette = \"Dark2\",\n        title = \"Species Occurrences\",\n        popup.vars = c(\n          \"Record Type\" = \"basisOfRecord\",\n          \"Year\" = \"year\",\n          \"Month\" = \"month\",\n          \"Elevation (m)\" = \"elevation\"\n        )\n      ) +\n      tm_shape(urban) +\n        tm_polygons(alpha = 0.7, title = \"Urban Areas\") +\n      tm_shape(elevation)+\n        tm_raster(alpha = 0.8, title = \"Elevation (m)\")\n    \n    \n  })\n  \n\n  \n}\n\nRun the app:\n\nshinyApp(ui = ui, server = server)\n\n\nTo learn more about shiny , there are a lot of great beginner lessons here. You can publicly host your shiny apps for free with services like shinyapps.io."
  },
  {
    "objectID": "getdata.html",
    "href": "getdata.html",
    "title": "Pulling Data",
    "section": "",
    "text": "Below is the code used to import all data sets used for this workshop. There are tons of R packages that allow you to connect to open-source spatial databases and pull spatial data right into your R session (without having to download and read in the data separately). Some packages require an API key before importing the data, which are free to sign up for."
  },
  {
    "objectID": "getdata.html#load-in-libraries",
    "href": "getdata.html#load-in-libraries",
    "title": "Pulling Data",
    "section": "Load in Libraries",
    "text": "Load in Libraries\nThis function checks if all packages are installed in your local system, if not it will install them, and then load all of them into your R session. If you don’t have any of these packages installed this step may take a little while.\n\npackageLoad <-\n  function(x) {\n    for (i in 1:length(x)) {\n      if (!x[i] %in% installed.packages()) {\n        install.packages(x[i])\n      }\n      library(x[i], character.only = TRUE)\n    }\n  }\n\npackageLoad(c(\"elevatr\", \"rgbif\", \"tidycensus\", \"tigris\", \"sf\", \"terra\", \"dplyr\"))\n\nFor this workshop we are focusing on the state of Colorado, so we first download some political boundaries to use to filter the raster layers we want returned. The tigris package allows you to directly download TIGER/Line shapefiles from the US Census Bureau.\n\n# download county shapefile for the state of Colorado\ncounties <- tigris::counties(state = \"CO\")"
  },
  {
    "objectID": "getdata.html#raster-data",
    "href": "getdata.html#raster-data",
    "title": "Pulling Data",
    "section": "Raster Data",
    "text": "Raster Data\nGet elevation data using the elevatr package. The function get_elev_raster() returns a raster digital elevation model (DEM) from the AWS Open Data Terrain Tiles. For this function you must supply a spatial object specifying the extent of the returned elevation raster and the resolution (specified by the zoom level z). We are importing elevation at ~ 1km resolution (~ 900 m).\n\n# get elevation at ~1km (864m)\nelevation <- get_elev_raster(counties, z = 7)\n\n# save elevation to file as a GeoTIFF\nterra::writeRaster(elevation, \"data/elevation_1km.tif\")\n\nFor this workshop we are also going to work with a land cover raster data set from the National Land Cover Database (NLCD). You can download NLCD data directly from R using the FedData package, however the most updated land cover data available is from 2011. For this course, NLCD 2019 CONUS data was downloaded to my local system from the MRLC website. The following code is how I read in and cleaned the land cover data for use in this workshop. Processing includes cropping the CONUS data set to the state of Colorado, and then aggregating (reducing the resolution) of the raster from 30m to ~1km (990m) to make processing and analysis quicker for the workshop.\n\n# Read in the raster (image) file\nland <- terra::rast('L:/Projects_active/EnviroScreen/data/NLCD/Land Cover/nlcd_2019_land_cover_l48_20210604.img') \n\n#transform the counties spatial object to match landcover so we can perform crop and mask operations\ncounties_aea <- st_transform(counties, crs(land))\n\n# crop the landcover to Colorado\nland_co <- land %>% \n  terra::crop(vect(counties_aea)) %>%\n  terra::mask(vect(counties_aea))\n\n\n#aggregate to ~1km for ease of processing/analysis in course\nland_co1km <- terra::aggregate(land_co, fact = 33, fun = \"modal\")\n\n\n# save processed raster file\nterra::writeRaster(land_co1km, filename = \"data/NLCD_CO.tif\", overwrite = TRUE)"
  },
  {
    "objectID": "getdata.html#point-data",
    "href": "getdata.html#point-data",
    "title": "Pulling Data",
    "section": "Point Data",
    "text": "Point Data\nWe will be working with some species occurrences in the form of point data (latitude/longitude). rgbif is a package that allows you to download species occurrences from the Global Biodiversity Information Facility (GBIF), a database of global species occurrences with over 2.2 billion records.\n\n\n\n\n\n\n\n\n\n\n\n\n\nElk\nYellow-Bellied Marmot\nWestern Tiger Salamander\n\n\n\n\n#make a string of species names to use in the 'occ_data' function\nspecies <- c(\"Cervus canadensis\", \"Marmota flaviventris\", \"Ambystoma mavortium\")\n\n#also make a string of common names to use for plotting later\ncommon_name <- c(\"Elk\", \"Yellow-bellied Marmot\", \"Western Tiger Salamander\")\n\n\n# write a for loop to extract occurrence data for each species\n\n# create an empty vector to store each species' downloaded occurrence data\nocc <- vector(\"list\", length = length(species)) \n\n\nfor(i in 1:length(occ)){\n  \n  occ[[i]] <-\n    occ_data(\n      scientificName = species[i],\n      hasCoordinate = TRUE,\n      geometry = st_bbox(counties),\n      limit = 2000\n    ) %>%\n    .$data #return just the data frame\n  \n  # add species name column as ID to use later\n  occ[[i]]$ID <- common_name[i]\n  \n  #clean by removing duplicate occurrences\n  occ[[i]] <-\n    occ[[i]] %>% distinct(decimalLatitude, decimalLongitude, .keep_all = TRUE) %>%\n    dplyr::select(Species = ID,\n           decimalLatitude,\n           decimalLongitude,\n           year,\n           month,\n           basisOfRecord) #only keep relevant variables\n  \n  \n  \n  print(i) # this prints each element once its finished so you can see the progress\n  \n}\n\n# Bind all data frames together\nocc <- bind_rows(occ) \n\n\nwrite_csv(occ, \"data/species_occ.csv\")"
  },
  {
    "objectID": "getdata.html#line-data",
    "href": "getdata.html#line-data",
    "title": "Pulling Data",
    "section": "Line Data",
    "text": "Line Data\nUse the tigris package to download linear water features (streams/rivers, braided streams, canals, ditches, artificial paths, and aqueducts) and roads for Larimer County.\n\nrivers <- linear_water(state = \"CO\", county = \"Larimer\")\n\nroads <- roads(state = \"CO\", county = \"Larimer\")\n\n# save the files\nst_write(rivers, \"data/rivers.shp\")\n\nst_write(roads, \"data/roads.shp\")"
  },
  {
    "objectID": "getdata.html#polygon-data",
    "href": "getdata.html#polygon-data",
    "title": "Pulling Data",
    "section": "Polygon Data",
    "text": "Polygon Data\nWe already imported a spatial polygon data set, counties. We are going to work with one more polygon data set from the tigris package which includes the boundaries of individual urban areas and clusters across Colorado. We have to do a little data cleaning to filter just urban areas within the state of Colorado.\n\nurban <- urban_areas() %>% \n  tidyr::separate(col = NAME10, sep = \", \", into = c(\"city\", \"state\")) %>% \n  dplyr::filter(state == \"CO\")\n\n#save the file\nst_write(urban, \"data/urban_areas.shp\")\n\nFinally, we are going to work with some census variables, namely total population and median household income. You can import census data with the tidycensus package, which does require an API key. You can obtain a key for free here. We import these variables using the get_acs() function (“acs” stands for American Community Survey), specifying the state of Colorado and to return data at the county level. We import just the county level data here as we will tie these attributes to our spatial data later, but you could also import these results as spatial objects by setting geometry = TRUE.\n\n# supply your unique API key\ncensus_api_key(\"PASTE YOUR API KEY HERE\")\n\n#import total population and median household income\ncensus <- get_acs(geography = \"county\", state = \"CO\", year = 2019,\n                  variables = c(\"B01003_001\", \"B19013_001\"), output = \"wide\")\n\n#clean the data\ncensus <- census %>% \n  dplyr::select(contains(\"E\")) %>% \n  rename(total_pop = B01003_001E, med_income = B19013_001E)\n\n#save it\nwrite_csv(census, \"data/census_data.csv\")"
  },
  {
    "objectID": "getdata.html#other-data-libraries",
    "href": "getdata.html#other-data-libraries",
    "title": "Pulling Data",
    "section": "Other Data Libraries",
    "text": "Other Data Libraries\nR’s collection of data retrieval libraries is extensive. We only use a few of them in this workshop, but I wanted to mention a few other packages that may be of interest:\n\n\n\nrnaturalearth\nNatural Earth spatial data\n\n\nrnoaa\nNOAA weather data\n\n\ndataRetrieval\nUSGS water data\n\n\nwdpar\nWorld Database on Protected Areas\n\n\nrgee\nuse Google Earth Engine (and connect to the entire data collection) in R\n\n\nnhdplusTools\nHydrographic data\n\n\nFedData\nSpatial data from several U.S. federal data sources, such as elevation, hydrography, soil, land cover, cropland, and climate data sets."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Geospatial",
    "section": "",
    "text": "This is the R for Geospatial course website where you will find all necessary material. This course is designed for individuals with a basic understanding of R and RStudio and an interest in working with spatial data. If you are new to R/RStudio, please review the R Basics page before the workshop, including the attached lessons/tutorials. Even if you have R experience, it is recommended to still review the introductory material to make sure everyone is on the same page by the start of the workshop.\nIn this workshop you will learn how to better handle, analyze, and visualize spatial data in R. You will also be exposed to various open-source spatial databases and learn how to access them directly from R to pull in data. The general breakdown of the workshop is as follows:\nLesson 1:\n\nIntroduction to spatial data formats (vector and raster)\nImporting, cleaning and manipulating spatial data\n\nLesson 2:\n\nSpatial analysis (e.g., buffer, intersect, extract, spatial joins, distance and area calculations)\nBasic plotting\n\nLesson 3:\n\nSpatial data visualization (static, dynamic, interactive)\nData sharing (e.g., Shiny, R Markdown)\n\nBefore Lesson 1 of the workshop you must go to the Getting Started page to complete all pre-workshop steps and set-up your workspace with the necessary software and packages. We will have set-up hours 30 minutes before the start of Lesson 1 if you need assistance, or you can email ccmothes@colostate.edu with any set-up issues."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Getting Started",
    "section": "",
    "text": "This is the set-up page for the R for Geospatial workshop. You must complete all steps on this page before the first day of the workshop. If you have any issues, we will have set-up hours 30 minutes before the start of Lesson 1, or you can email ccmothes@colostate.edu beforehand.\nThis course is also designed for individuals with a basic understanding of R and RStudio. If you are new to R/RStudio, please read through the R Basics page, including the attached lessons/tutorials. Even if you do have experience with R, this page is a good refresher before diving into this course content."
  },
  {
    "objectID": "setup.html#install-r-and-rstudio",
    "href": "setup.html#install-r-and-rstudio",
    "title": "Getting Started",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nR is an open source language and software environment for statistical analysis and graphics (plus so much more!). You must first download the R software (for free) here: https://www.r-project.org/.\nNote: If you already have R installed, we recommend that you have at least version 4.0.0. or greater. At minimum you will need R version > 3.5.0 to use the required packages.\nRStudio is a (also free) R Integrated Development Environment (IDE) that provides a built-in editor and other advantages such as version control and project management. Once you have the R software installed on your computer, you can install RStudio Desktop here: https://www.rstudio.com/products/rstudio/."
  },
  {
    "objectID": "setup.html#package-installation",
    "href": "setup.html#package-installation",
    "title": "Getting Started",
    "section": "Package Installation",
    "text": "Package Installation\nWhile the R software comes with many pre-loaded functions (referred to as ‘base R’ functions), there are thousands of R packages that provide additional reusable R functions. In order to use these functions you need to first install the package to your local machine using the install.packages() function. Once a package is installed on your computer you don’t need to install it again (but you may have to update it). Anytime you want to use the package in a new R session you can load it with the library() function.\nThe packages we are using for this workshop are listed below, and you can read more about them by clicking on the hyperlink:\n\n\n\nData Retrieval\nData Wrangling\nData Visualization\nProcessing Spatial Data\nSharing Data\n\n\ntidycensus\ntigris\nrgbif\ndplyr\ntidyr\nreadr\nggplot2\ntmap\nsf\nterra\nrmarkdown\nshiny\n\n\n\nWe will be working in RStudio this entire course, so after you have installed both R and RStudio, open a new session of RStudio. If you are new to working with RStudio visit the R Basics page for an overview first.\nTo make this a little more reproducible, we are going to use a function throughout the course that checks if you need to install these packages on your system or not, installs them if needed, and then loads all the libraries into your R session. You can copy the code below by clicking the clipboard icon in the upper-right corner and run it in your console, or save it in a script. We will walk through saving it as a script in Lesson 1 to reuse it throughout the course.\n\npackageLoad <-\n  function(x) {\n    for (i in 1:length(x)) {\n      if (!x[i] %in% installed.packages()) {\n        install.packages(x[i])\n      }\n      library(x[i], character.only = TRUE)\n    }\n  }\n\nAfter you run this code, you should now have packageLoad as a function in your Environment tab (upper-right corner of RStudio). Now you can feed in a list of all the packages we need for this workshop and let R do the rest! You can copy this chunk of code and run in your console:\n\npackageLoad(c(\"tidycensus\", \"tigris\", \"rgbif\", \"sf\", \"terra\",\n              \"dplyr\", \"tidyr\", \"readr\", \"ggplot2\", \"tmap\",\n              \"rmarkdown\", \"shiny\"))\n\nThis may take a while to run, and you will see a lot of text being printed to your console. There may even be some warnings, but you can ignore most of this text unless you see ‘error’ anywhere. If you get errors and can’t figure out how to troubleshoot, please send me an email (ccmothes@colostate.edu) with a screenshot of the error and which package(s) you are having trouble installing. You can also join 30 mins before the first day of the workshop to troubleshoot in person.\nTo make sure you have all the needed packages installed and loaded, you can run this line of code and make sure you see all the packages in the table above printed to your console:\n\n(.packages())"
  }
]